{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0658e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = './mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
    "import numpy as np\n",
    "from matplotlib.pylab import plt\n",
    "#import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ff1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transforms(a, w):\n",
    "    \n",
    "    for W in w:\n",
    "        \n",
    "        bias = W[0]\n",
    "        mult = np.dot(W[1:].T, a)\n",
    "        a = bias + mult.T\n",
    "        a = np.tanh(a).T \n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d34a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, theta):\n",
    "    \n",
    "    f = feature_transforms(x, theta[0])\n",
    "    \n",
    "    a = theta[1][0] + np.dot(f.T, theta[1][1:])\n",
    "    \n",
    "    return a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c866a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initializer(layer_sizes, scale):\n",
    "    \n",
    "    weights = []\n",
    "    \n",
    "    for k in range(len(layer_sizes)-1):\n",
    "        U_k = layer_sizes[k]\n",
    "        U_k_plus_1 = layer_sizes[k+1]\n",
    "                \n",
    "        weight = scale*np.random.randn(U_k+1, U_k_plus_1)\n",
    "        \n",
    "        weights.append(weight)\n",
    "        \n",
    "    theta_init = [weights[:-1], weights[-1]]\n",
    "    \n",
    "    return np.array(theta_init, dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d7d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(a, x, y):\n",
    "    \n",
    "    N, D = x.shape\n",
    "    exp_a = np.exp(a - np.max(a))\n",
    "    softmax =  exp_a / exp_a.sum(axis=0, keepdims=True)\n",
    "        \n",
    "    \n",
    "    loss = np.sum(y - np.log(softmax))\n",
    "    loss /= N\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a2923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 2)\n",
      "(96, 2)\n",
      "[0.24539 0.81725] [1 0]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(datapath + '2_eggs.csv', delimiter=',')\n",
    "x = data[:2,:].T.astype(float)\n",
    "y = data[2,:][np.newaxis,:].astype(int)\n",
    "\n",
    "y[y==-1] = 0\n",
    "\n",
    "\n",
    "y1 = np.zeros((y.size, y.max()+1)).astype(int)\n",
    "y1[np.arange(y.size),y] = 1\n",
    "\n",
    "y = y1\n",
    "\n",
    "print(np.shape(x))\n",
    "print(np.shape(y))\n",
    "print(x[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72d2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(5,5))\n",
    "#plt.scatter(x[:,0], x[:,1], c=y, s=20, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34523030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import nn, utils\n",
    "from autograd.engine import Scalar\n",
    "from autograd.visualize import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f95f5e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP([Layer([ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2),ReLUNeuron(2)]),Layer([ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10)]),Layer([ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10)]),Layer([ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10),ReLUNeuron(10)]),Layer([LinearNeuron(10),LinearNeuron(10)])])\n",
      "\n",
      "Number of trainable parameters: 382\n"
     ]
    }
   ],
   "source": [
    "model = nn.MLP(2,[10,10,10,10,2])\n",
    "print(model)\n",
    "print(f\"\\nNumber of trainable parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6434613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def calculate_loss(batch_size=None):\n",
    "    if not batch_size:\n",
    "        Xb, yb = x, y\n",
    "    else:\n",
    "        random_indexes = np.random.permutation(x.shape[0])[:batch_size]\n",
    "        Xb, yb = x[random_indexes], y[random_indexes]\n",
    "    \n",
    "    # initialize inputs as Scalars\n",
    "    inputs = [list(map(Scalar, row)) for row in Xb]\n",
    "    # initialize labels as Scalars\n",
    "    #yb = [list(map(Scalar, row)) for row in yb]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # forward propagation\n",
    "    outputs = list(map(model, inputs))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # svm \"max-margin\" loss\n",
    "    #losses = utils.svm_max_margin_loss(outputs, yb)\n",
    "    data_loss = utils.softmax_loss(outputs, yb)\n",
    "    #data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 regularization (adding sum of squares of weights to penalize large weights, times hyperparameter alpha)\n",
    "    #reg_loss = utils.l2_regularization(model, alpha=1e-4)\n",
    "    #total_loss = data_loss + reg_loss\n",
    "    total_loss = data_loss\n",
    "\n",
    "    # calculate accuracy\n",
    "    outputs = [[x[0].value, x[1].value] for x in outputs]\n",
    "    #print(outputs[0])\n",
    "    predicted = np.argmax(outputs, axis = 1)\n",
    "    predicted1 = np.zeros((predicted.size, 2)).astype(int)\n",
    "    predicted1[np.arange(predicted.size),predicted] = 1\n",
    "    predicted = predicted1\n",
    "    accuracy = [(y_i[0] == output_i[0] and y_i[1] == output_i[1]) for y_i, output_i in zip(yb, predicted)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af53d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs=100, debug=False):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, accuracy = calculate_loss()\n",
    "        \n",
    "        # zero out previous gradients for next iteration of backpropagation\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # backprop\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # update parameters (stochastic gradient descent with learning rate decay)\n",
    "        #learning_rate = 1.0 - 0.9*epoch/100\n",
    "        learning_rate = 0.1\n",
    "        \n",
    "        for parameter in model.parameters():\n",
    "            if debug:\n",
    "                print('before',parameter.value,'grad',parameter.grad)\n",
    "            parameter.value -= learning_rate * parameter.grad\n",
    "            if debug:\n",
    "                print('after',parameter.value)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {total_loss.value}, Accuracy: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a6a9576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.36919848 4.83709935]\n",
      " [5.41850789 4.84329011]\n",
      " [5.49134542 4.84830597]\n",
      " [5.63106373 4.85309595]\n",
      " [5.73353834 4.85463423]\n",
      " [5.81593904 4.83740619]\n",
      " [5.83052688 4.82501103]\n",
      " [5.68355795 4.82460602]\n",
      " [5.61722975 4.8395593 ]\n",
      " [5.51399565 4.83372221]\n",
      " [5.40236718 4.82698805]\n",
      " [5.37707376 4.83129925]\n",
      " [5.45745328 4.83733041]\n",
      " [5.58287066 4.84441845]\n",
      " [5.69987396 4.84171105]\n",
      " [5.56496464 4.84128037]\n",
      " [5.73881243 4.63048443]\n",
      " [5.83672629 4.57289574]\n",
      " [5.77565514 4.32376016]\n",
      " [5.4933471  4.21687433]\n",
      " [5.43070524 4.17187357]\n",
      " [5.48853873 4.20508494]\n",
      " [5.66590247 4.37990029]\n",
      " [5.67669845 4.3778605 ]\n",
      " [5.79524216 4.38509597]\n",
      " [5.91009182 4.58682892]\n",
      " [5.79871674 4.65523293]\n",
      " [5.70583622 4.69560109]\n",
      " [5.81497664 4.62135326]\n",
      " [5.7949183  4.35734499]\n",
      " [5.59065269 4.30383719]\n",
      " [5.57921042 4.30223162]\n",
      " [4.1633568  5.82155115]\n",
      " [4.28461234 5.83093892]\n",
      " [4.31970041 5.81891376]\n",
      " [4.27286196 5.81265209]\n",
      " [4.40201821 5.81496272]\n",
      " [4.26355154 5.82218329]\n",
      " [4.56955611 5.79548554]\n",
      " [4.51254964 5.81235273]\n",
      " [4.75457852 5.79703784]\n",
      " [4.64896561 5.77856121]\n",
      " [4.80919555 5.72717066]\n",
      " [4.89084942 5.74549295]\n",
      " [4.7530342  5.79846306]\n",
      " [4.98214095 5.58109829]\n",
      " [4.98743625 5.67555555]\n",
      " [4.97647685 5.78369733]\n",
      " [4.99764964 5.82157209]\n",
      " [5.08203023 5.72829878]\n",
      " [5.06331989 5.73377356]\n",
      " [4.81149827 5.51791616]\n",
      " [4.56913715 5.18394624]\n",
      " [4.69076071 5.40903234]\n",
      " [4.41584542 4.94297896]\n",
      " [4.17940204 4.59651775]\n",
      " [4.17318272 4.76440236]\n",
      " [4.01544991 4.4102165 ]\n",
      " [4.2301345  4.98869939]\n",
      " [4.19296799 4.95395377]\n",
      " [4.02520096 4.57900262]\n",
      " [4.38398848 5.11339583]\n",
      " [4.45467892 5.17529888]\n",
      " [4.76339544 5.29450233]\n",
      " [4.80739767 5.50746593]\n",
      " [4.61890553 5.67214046]\n",
      " [4.49773495 5.73564845]\n",
      " [4.38017299 5.75919392]\n",
      " [4.49715802 5.76092733]\n",
      " [4.32873096 5.78793714]\n",
      " [4.46662741 5.77874077]\n",
      " [4.59499492 5.74010268]\n",
      " [4.52257664 5.74461626]\n",
      " [4.5672972  5.66497555]\n",
      " [4.16415975 5.81735283]\n",
      " [4.24137621 5.82442072]\n",
      " [4.22895574 5.82056104]\n",
      " [4.15841303 5.813259  ]\n",
      " [4.32223467 5.82800169]\n",
      " [4.30486499 5.82798854]\n",
      " [4.44753485 5.84154279]\n",
      " [4.42626777 5.83724466]\n",
      " [4.54934971 5.84855951]\n",
      " [4.6052558  5.86078885]\n",
      " [4.66520039 5.86269903]\n",
      " [4.79564858 5.86801031]\n",
      " [4.66251696 5.85910138]\n",
      " [4.83551802 5.8769807 ]\n",
      " [4.85126158 5.85323319]\n",
      " [4.94330127 5.86663011]\n",
      " [5.02901867 5.87057053]\n",
      " [5.13773702 5.84978666]\n",
      " [5.02752984 5.71410581]\n",
      " [4.88540782 5.5891405 ]\n",
      " [4.96038101 5.88148798]\n",
      " [5.02377412 5.88549591]]\n",
      "[[Scalar(value=0.86, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.81, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.74, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.60, grad=0.00), prev_op=+, Scalar(value=0.05, grad=0.00), prev_op=+], [Scalar(value=0.50, grad=0.00), prev_op=+, Scalar(value=0.05, grad=0.00), prev_op=+], [Scalar(value=0.41, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.40, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.55, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.61, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.72, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.83, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.85, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.77, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.65, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.53, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.66, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.49, grad=0.00), prev_op=+, Scalar(value=0.28, grad=0.00), prev_op=+], [Scalar(value=0.39, grad=0.00), prev_op=+, Scalar(value=0.33, grad=0.00), prev_op=+], [Scalar(value=0.45, grad=0.00), prev_op=+, Scalar(value=0.58, grad=0.00), prev_op=+], [Scalar(value=0.74, grad=0.00), prev_op=+, Scalar(value=0.69, grad=0.00), prev_op=+], [Scalar(value=0.80, grad=0.00), prev_op=+, Scalar(value=0.73, grad=0.00), prev_op=+], [Scalar(value=0.74, grad=0.00), prev_op=+, Scalar(value=0.70, grad=0.00), prev_op=+], [Scalar(value=0.56, grad=0.00), prev_op=+, Scalar(value=0.53, grad=0.00), prev_op=+], [Scalar(value=0.55, grad=0.00), prev_op=+, Scalar(value=0.53, grad=0.00), prev_op=+], [Scalar(value=0.43, grad=0.00), prev_op=+, Scalar(value=0.52, grad=0.00), prev_op=+], [Scalar(value=0.32, grad=0.00), prev_op=+, Scalar(value=0.32, grad=0.00), prev_op=+], [Scalar(value=0.43, grad=0.00), prev_op=+, Scalar(value=0.25, grad=0.00), prev_op=+], [Scalar(value=0.52, grad=0.00), prev_op=+, Scalar(value=0.21, grad=0.00), prev_op=+], [Scalar(value=0.41, grad=0.00), prev_op=+, Scalar(value=0.28, grad=0.00), prev_op=+], [Scalar(value=0.43, grad=0.00), prev_op=+, Scalar(value=0.55, grad=0.00), prev_op=+], [Scalar(value=0.64, grad=0.00), prev_op=+, Scalar(value=0.60, grad=0.00), prev_op=+], [Scalar(value=0.65, grad=0.00), prev_op=+, Scalar(value=0.60, grad=0.00), prev_op=+], [Scalar(value=1.07, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.94, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.91, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.96, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.83, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.97, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.66, grad=0.00), prev_op=+, Scalar(value=0.11, grad=0.00), prev_op=+], [Scalar(value=0.72, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.47, grad=0.00), prev_op=+, Scalar(value=0.11, grad=0.00), prev_op=+], [Scalar(value=0.58, grad=0.00), prev_op=+, Scalar(value=0.13, grad=0.00), prev_op=+], [Scalar(value=0.42, grad=0.00), prev_op=+, Scalar(value=0.18, grad=0.00), prev_op=+], [Scalar(value=0.34, grad=0.00), prev_op=+, Scalar(value=0.16, grad=0.00), prev_op=+], [Scalar(value=0.48, grad=0.00), prev_op=+, Scalar(value=0.11, grad=0.00), prev_op=+], [Scalar(value=0.25, grad=0.00), prev_op=+, Scalar(value=0.32, grad=0.00), prev_op=+], [Scalar(value=0.24, grad=0.00), prev_op=+, Scalar(value=0.23, grad=0.00), prev_op=+], [Scalar(value=0.25, grad=0.00), prev_op=+, Scalar(value=0.12, grad=0.00), prev_op=+], [Scalar(value=0.23, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.15, grad=0.00), prev_op=+, Scalar(value=0.18, grad=0.00), prev_op=+], [Scalar(value=0.17, grad=0.00), prev_op=+, Scalar(value=0.17, grad=0.00), prev_op=+], [Scalar(value=0.42, grad=0.00), prev_op=+, Scalar(value=0.39, grad=0.00), prev_op=+], [Scalar(value=0.66, grad=0.00), prev_op=+, Scalar(value=0.72, grad=0.00), prev_op=+], [Scalar(value=0.54, grad=0.00), prev_op=+, Scalar(value=0.50, grad=0.00), prev_op=+], [Scalar(value=0.81, grad=0.00), prev_op=+, Scalar(value=0.96, grad=0.00), prev_op=+], [Scalar(value=1.05, grad=0.00), prev_op=+, Scalar(value=1.31, grad=0.00), prev_op=+], [Scalar(value=1.06, grad=0.00), prev_op=+, Scalar(value=1.14, grad=0.00), prev_op=+], [Scalar(value=1.21, grad=0.00), prev_op=+, Scalar(value=1.50, grad=0.00), prev_op=+], [Scalar(value=1.00, grad=0.00), prev_op=+, Scalar(value=0.92, grad=0.00), prev_op=+], [Scalar(value=1.04, grad=0.00), prev_op=+, Scalar(value=0.95, grad=0.00), prev_op=+], [Scalar(value=1.20, grad=0.00), prev_op=+, Scalar(value=1.33, grad=0.00), prev_op=+], [Scalar(value=0.85, grad=0.00), prev_op=+, Scalar(value=0.79, grad=0.00), prev_op=+], [Scalar(value=0.77, grad=0.00), prev_op=+, Scalar(value=0.73, grad=0.00), prev_op=+], [Scalar(value=0.47, grad=0.00), prev_op=+, Scalar(value=0.61, grad=0.00), prev_op=+], [Scalar(value=0.42, grad=0.00), prev_op=+, Scalar(value=0.40, grad=0.00), prev_op=+], [Scalar(value=0.61, grad=0.00), prev_op=+, Scalar(value=0.23, grad=0.00), prev_op=+], [Scalar(value=0.73, grad=0.00), prev_op=+, Scalar(value=0.17, grad=0.00), prev_op=+], [Scalar(value=0.85, grad=0.00), prev_op=+, Scalar(value=0.15, grad=0.00), prev_op=+], [Scalar(value=0.73, grad=0.00), prev_op=+, Scalar(value=0.14, grad=0.00), prev_op=+], [Scalar(value=0.90, grad=0.00), prev_op=+, Scalar(value=0.12, grad=0.00), prev_op=+], [Scalar(value=0.76, grad=0.00), prev_op=+, Scalar(value=0.13, grad=0.00), prev_op=+], [Scalar(value=0.63, grad=0.00), prev_op=+, Scalar(value=0.17, grad=0.00), prev_op=+], [Scalar(value=0.71, grad=0.00), prev_op=+, Scalar(value=0.16, grad=0.00), prev_op=+], [Scalar(value=0.66, grad=0.00), prev_op=+, Scalar(value=0.24, grad=0.00), prev_op=+], [Scalar(value=1.06, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.99, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=1.00, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=1.07, grad=0.00), prev_op=+, Scalar(value=0.09, grad=0.00), prev_op=+], [Scalar(value=0.91, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.92, grad=0.00), prev_op=+, Scalar(value=0.08, grad=0.00), prev_op=+], [Scalar(value=0.78, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.80, grad=0.00), prev_op=+, Scalar(value=0.07, grad=0.00), prev_op=+], [Scalar(value=0.68, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.62, grad=0.00), prev_op=+, Scalar(value=0.04, grad=0.00), prev_op=+], [Scalar(value=0.56, grad=0.00), prev_op=+, Scalar(value=0.04, grad=0.00), prev_op=+], [Scalar(value=0.43, grad=0.00), prev_op=+, Scalar(value=0.04, grad=0.00), prev_op=+], [Scalar(value=0.57, grad=0.00), prev_op=+, Scalar(value=0.05, grad=0.00), prev_op=+], [Scalar(value=0.39, grad=0.00), prev_op=+, Scalar(value=0.03, grad=0.00), prev_op=+], [Scalar(value=0.38, grad=0.00), prev_op=+, Scalar(value=0.05, grad=0.00), prev_op=+], [Scalar(value=0.29, grad=0.00), prev_op=+, Scalar(value=0.04, grad=0.00), prev_op=+], [Scalar(value=0.20, grad=0.00), prev_op=+, Scalar(value=0.03, grad=0.00), prev_op=+], [Scalar(value=0.09, grad=0.00), prev_op=+, Scalar(value=0.06, grad=0.00), prev_op=+], [Scalar(value=0.20, grad=0.00), prev_op=+, Scalar(value=0.19, grad=0.00), prev_op=+], [Scalar(value=0.34, grad=0.00), prev_op=+, Scalar(value=0.32, grad=0.00), prev_op=+], [Scalar(value=0.27, grad=0.00), prev_op=+, Scalar(value=0.02, grad=0.00), prev_op=+], [Scalar(value=0.21, grad=0.00), prev_op=+, Scalar(value=0.02, grad=0.00), prev_op=+]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "node value must be a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9675/2207533955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9675/1745035277.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, debug)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# zero out previous gradients for next iteration of backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9675/96901328.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# svm \"max-margin\" loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#losses = utils.svm_max_margin_loss(outputs, yb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m#data_loss = sum(losses) * (1.0 / len(losses))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/EE_435/autograd/utils.py\u001b[0m in \u001b[0;36msoftmax_loss\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/EE_435/autograd/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/EE_435/autograd/engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, parent_nodes, prev_op)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"node value must be a scalar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: node value must be a scalar"
     ]
    }
   ],
   "source": [
    "train(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a10734",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [2, 10, 10, 10, 1]\n",
    "\n",
    "w = network_initializer(layer_sizes, 1)\n",
    "print(w[1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f089f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import elementwise_grad \n",
    "\n",
    "\n",
    "def fit(x, y,w, steps = 1000, lr = 0.001, comp_wise=False):\n",
    "    \n",
    "    gradient = elementwise_grad(model)\n",
    "    \n",
    "    values = []\n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        fw,gw = gradient(x, w)\n",
    "        \n",
    "        #print(fw1)\n",
    "                \n",
    "        out = model(x, w)\n",
    "        \n",
    "        loss = loss_func(out, x, y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if comp_wise == False:\n",
    "            gw_norm = np.linalg.norm(gw)\n",
    "            if gw_norm == 0 :\n",
    "                gw_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n",
    "            gw /= gw_norm\n",
    "        \n",
    "        else:\n",
    "            comp_norm = np.abs(gw) + 10**(-8)\n",
    "            gw /= comp_norm\n",
    "        \n",
    "        \n",
    "        print(w.shape)\n",
    "        print(gw.shape)\n",
    "        print(fw.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        w = w - gw*step\n",
    "        \n",
    "#         values.append(fw)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08dc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(x, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5200eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95990a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e8e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da949a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
